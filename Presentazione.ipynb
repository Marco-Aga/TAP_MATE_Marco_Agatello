{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24aceb47-d6c7-4590-a84c-3bbb11e2861c",
   "metadata": {},
   "source": [
    "# MATE - Monitoraggio Avanzato di Temperature e Effetti\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4fb171-b6f9-4966-996b-002ca93f7fd3",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"Img/Varie/MATE-logo.jpeg\" align=\"center\" width=\"400\" style=\"height:auto;\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14aef723-67c0-4272-9ef2-4919d9fee700",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Introduzione\n",
    "### Descrizione del Progetto\n",
    "Il progetto **MATE - Monitoraggio Avanzato di Temperature e Effetti** è stato creato per monitorare in modo semplice ed efficace le condizioni ambientali, come temperatura e umidità, sia per interni che esterni. Grazie a tecnologie avanzate, trasforma i dati grezzi in informazioni utili, facilmente accessibili e visibili in tempo reale. \n",
    "#### Panoramica del Sistema\n",
    "Il cuore di MATE è un sensore di temperatura e umidità che fornisce dati sia in tempo reale che storici. \n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"Img/Varie/Meter.jpg\" width=\"400\" style=\"height:auto;\" />\n",
    "</div> \n",
    "   \n",
    "Questo sistema, semplice da configurare e utilizzare, garantisce dati accurati e costanti. Ecco cosa offre:\n",
    "- **Dati Storici**: Con una banca dati di oltre tre anni di misurazioni, potrai analizzare le tendenze nel tempo e identificare potenziali miglioramenti ambientali.\n",
    "- **Dati in Tempo Reale**: Le misurazioni attuali ti permettono di reagire immediatamente a qualsiasi variazione climatica, garantendo un controllo completo sulle condizioni attuali.\n",
    "\n",
    "Immagina di avere il pieno controllo del tuo ambiente, che si tratti di una serra, un ufficio o la tua abitazione, con la capacità di ottimizzare il riscaldamento, il raffreddamento e l'isolamento grazie alle informazioni raccolte e analizzate da MATE.\n",
    "\n",
    "#### Obiettivi del Progetto\n",
    "\n",
    "Gli obiettivi principali del progetto sono:\n",
    "- **Raccolta e Integrazione dei Dati**: Utilizzando Logstash per prelevare i dati storici da file CSV e i dati in tempo reale tramite API. Integrare entrambi i set di misurazioni in un flusso unico di dati.\n",
    "- **Gestione dei Dati**: Attravero Kafka per gestire il flusso continuo delle misurazioni.\n",
    "- **Elaborazione dei Dati**: Con l`uso di Spark per l'elaborazione e l'arricchimento dei dati, aggiungendo informazioni contestuali come la stagione e il momento della giornata, cioè notte o giorno della misurazione.\n",
    "- **Indicizzazione e Ricerca**: Caricare i dati elaborati in Elasticsearch per permettere una ricerca e un'analisi efficiente.\n",
    "- **Visualizzazione dei Dati**: Creare dashboard e grafici interattivi con Kibana per fornire una visualizzazione dettagliata delle temperature e dell'umidità, evidenziando le tendenze e le variazioni.\n",
    "\n",
    "#### Rilevanza del Progetto\n",
    "\n",
    "Il sistema di monitoraggio avanzato offerto da **MATE** non è solo un insieme di tecnologie, ma una soluzione pratica e potente per gestire le condizioni climatiche in ambienti diversi. La capacità di monitorare temperatura e umidità, sia all'interno che all'esterno, ha un impatto significativo su molteplici settori:\n",
    "- **Analisi Climatiche**: Comprendere le variazioni climatiche nel tempo e identificare modelli stagionali.\n",
    "- **Gestione Ambientale**: Monitorare e gestire le condizioni ambientali per applicazioni in agricoltura, edilizia e altre aree sensibili al clima.\n",
    "- **Monitoraggio degli Ambienti Interni**: Grazie al fatto che il sensore è di nostra proprietà, è possibile utilizzarlo per misurare le condizioni ambientali in luoghi interni. Questo permette di:\n",
    "  - **Verificare l'Esistenza di Umidità eccessiva**: Identificare e monitorare problemi di umidità che potrebbero causare danni o problemi di salute.\n",
    "  - **Valutare l'Isolamento delle Stanze**: Analizzare l'efficacia dell'isolamento termico delle stanze, contribuendo a ottimizzare il comfort abitativo e l'efficienza energetica.\n",
    "  - **Controllare le Condizioni Ambientali**: Valutare e migliorare le condizioni ambientali in spazi come uffici, abitazioni e magazzini.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb3bcd1",
   "metadata": {},
   "source": [
    "## 2. Tecnologie e Strumenti Utilizzati\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"Img/Diagrammi/Diagramma.png\"  style=\"height:auto;\" />\n",
    "</div> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1205957b",
   "metadata": {},
   "source": [
    "\n",
    "### Docker Compose\n",
    "Il progetto **MATE** sfrutta **Docker Compose**, uno strumento essenziale per la gestione e l'orchestrazione di applicazioni multi-conteiner Docker. Docker Compose semplifica la definizione, la configurazione e la gestione di ambienti complessi, grazie a un file di configurazione YAML che descrive tutti i servizi necessari per il progetto. Ma il principale vantaggio di adoperare docker compose sta nella sua facilita di portabilita, grazie al fatto di copiare il file YAML da un dispositivo ad un altro\n",
    "\n",
    "**Vantaggi :**\n",
    "\n",
    "- **Facilità di Portabilità**: Uno dei principali vantaggi di Docker Compose, come già accenato, è la sua capacità di rendere l'intero ambiente di sviluppo e produzione altamente portabile. Il file di configurazione YAML (`docker-compose.yaml`) contiene tutte le definizioni necessarie per i servizi, le reti (non usato nel progetto) e i volumi. Questo file può essere facilmente copiato da un dispositivo a un altro, permettendo di replicare l'ambiente di sviluppo o produzione su qualsiasi macchina con Docker installato. Questo garantisce coerenza e riduce i problemi di compatibilità tra ambienti diversi.\n",
    "\n",
    "- **Definizione dei Servizi**: Con Docker Compose, tutti i servizi necessari per il progetto, come Logstash, Kafka, Spark, Elasticsearch e Kibana, sono definiti in un unico file. Questo approccio centralizzato semplifica la configurazione e la gestione dell'intero stack tecnologico.\n",
    "\n",
    "- **Gestione Semplificata**: Docker Compose permette di avviare, fermare e gestire tutti i servizi del progetto con comandi semplici come `docker compose up` e `docker compose down`. Questo rende il processo di sviluppo e testing molto più efficiente e meno incline a errori manuali.\n",
    "\n",
    "- **Configurazione Centralizzata**: Le configurazioni, come porte, variabili d'ambiente e volumi, sono centralizzate nel file YAML, facilitando la modifica e la gestione delle impostazioni per tutti i servizi. Basti pensare come nel progetto per il servizio Spark sono state definite diverese variabili d' ambiente per semplificare l'esperienza utente \n",
    "\n",
    "- **Health Check**: Gli health check sono stati aggiunti ai servizi critici nel file di configurazione. Questo garantisce che i servizi come **Kafka** ed **Elasticsearch** siano completamente pronti prima che i servizi dipendenti, come `spark-app`, vengano avviati. Questo approccio aiuta a prevenire errori di connessione e migliora l'affidabilità complessiva del sistema. Prima dell'uso di questo strumento, non era raro che l'avvio di **Kafka** terminasse con un errore derivato dal fatto che **Zookeeper** non si fosse ancora avviato del tutto.\n",
    "```yaml\n",
    "    zookeeper:\n",
    "        #....\n",
    "        healthcheck:\n",
    "        # Definsico un healthcheck per il container zookeeper che controlla se il servizio è raggiungibile\n",
    "        test: [\"CMD\", \"nc\", \"-z\", \"localhost\", \"2181\"]\n",
    "        interval: 10s\n",
    "        timeout: 10s\n",
    "        retries: 15\n",
    "    \n",
    "    kafka:\n",
    "        #....\n",
    "        depends_on:\n",
    "        # Kafka aspetta che Zookeeper sia pronto grazie all'health check di Zookeeper\n",
    "        zookeeper:\n",
    "            condition: service_healthy\n",
    "```\n",
    "Ovviamente, questo in teoria. Nella pratica, si spera che l'esecuzione di `docker compose up` up non fallisca o che i container si avviino e poi escano con uno stato di errore di cui non si capisce il motivo. Inoltre, potrebbero sorgere problemi legati alla memoria insufficiente per mantenere in esecuzione tutti i servizi contemporaneamente.\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"Img/Meme/meme_docker_compose.jpg\" width=\"400\" style=\"height:auto;\" />\n",
    "</div>   \n",
    "\n",
    "Anche il principio di portabilità è teorico, perché nella pratica non è raro incorrere nel problema che un file di configurazione YAML che funziona su una macchina non funzioni su un'altra.\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"Img/Meme/meme_docker_compose_2.png\" width=\"400\" style=\"height:auto;\" />\n",
    "</div>   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd19cfc",
   "metadata": {},
   "source": [
    "### Logstash\n",
    "\n",
    "Il secondo strumento utilizzato per il progetto **MATE** è **Logstash**, il servizio che si occupa del **Data Ingestion**. Nel nostro caso, Logstash preleva i dati dal file CSV, effettua richieste API al server del termometro e convoglia tutti i dati raccolti direttamente a **Kafka**.\n",
    "\n",
    "Inoltre, Logstash ha permesso di modificare la struttura dei messaggi per garantire una certa uniformità tra i dati ottenuti dalle API e quelli del CSV. Ad esempio, le chiamate API non forniscono l'informazione del timestamp, a differenza del CSV. In tal caso, è stato sufficiente applicare un filtro per utilizzare il timestamp che Logstash fornisce di default.\n",
    "\n",
    "```properties\n",
    "# Filtro per i dati provenienti dall'input HTTP\n",
    "if [body] {\n",
    "  mutate {\n",
    "    rename => { \"[body][temperature]\" => \"Temperature_Celsius\" }\n",
    "    rename => { \"[body][humidity]\" => \"Relative_Humidity\" }\n",
    "    rename => { \"@timestamp\" => \"Timestamp\" }\n",
    "    remove_field => [\"@version\", \"body\", \"event\", \"statusCode\", \"message\"]\n",
    "  }\n",
    "}\n",
    "```  \n",
    "Un altro esempio riguarda il fatto che il CSV fornisce le misurazioni utilizzando il fuso orario locale, mentre i dati, per essere elaborati correttamente su **Elasticsearch**, devono essere convertiti nel fuso orario UTC. Nota bene, il codice che segue precede quello che conferisce il campo di Timestamp alle chiamate API, per cui questo filtro non si applica ai dati API.\n",
    " \n",
    "```properties \n",
    "  csv {\n",
    "      separator => \",\"\n",
    "      columns => [\"Timestamp\", \"Temperature_Celsius\", \"Relative_Humidity\"]\n",
    "      skip_header => true\n",
    "  }\n",
    "  date {\n",
    "      match => [\"Timestamp\", \"YYYY-MM-dd HH:mm\"]\n",
    "      timezone => \"Europe/Rome\"\n",
    "      target => \"Timestamp\"\n",
    "  }\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22ad167",
   "metadata": {},
   "source": [
    "### Kafka\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"Img/Diagrammi/Diagramma_Kafka.png\"  style=\"height:auto;\" />\n",
    "</div> \n",
    "  \n",
    "Nel progetto **MATE**, **Kafka** ricopre un ruolo cruciale come sistema di **Data Streaming** per la gestione dei dati in tempo reale e di quelli storici raccolti dal sensore attraverso l'uso di **Logstash**. Kafka funge da \"ponte\" tra **Logstash**, che si occupa della raccolta dei dati, e **Spark**, che li elabora e li arricchisce.\n",
    "\n",
    "Kafka è stato scelto per la sua capacità di gestire grandi volumi di dati in modo affidabile e scalabile. Ogni dato, sia proveniente dai file CSV (storici) che dalle API in tempo reale, viene inviato a Kafka in un apposito **topic**, chiamato *\"stream_input\"*, che funge da contenitore per i messaggi. Un topic può essere considerato come una coda temporanea in cui i dati vengono memorizzati.\n",
    "\n",
    "Kafka consente una comunicazione asincrona tra i componenti del progetto: **Logstash** agisce come **producer**, inviando i dati a Kafka, mentre **Spark** agisce come **consumer**, prelevando i dati dai topic e processandoli in tempo reale.\n",
    "\n",
    "Grazie al sistema di **persistenza** di Kafka, i dati restano memorizzati nei topic finché non vengono consumati, garantendo che nessun dato venga perso, anche in caso di guasti temporanei del sistema o di un collo di bottiglia dovuto a un'elaborazione più lenta da parte di **Spark**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a572e35",
   "metadata": {},
   "source": [
    "\n",
    "### Spark\n",
    "\n",
    "Nel progetto **MATE**, **Spark** rappresenta il cuore del progetto, gestendo il **Data Processing**. Come già accennato, Spark elabora i dati prelevati da **Kafka** e li invia a **Elasticsearch**. Le principali funzioni di Spark nel progetto sono:\n",
    "\n",
    "1. **Consumatore di Dati**: Spark agisce come consumatore dei dati dal topic di Kafka denominato \"stream_input\". Questo significa che Spark si collega a Kafka e preleva i dati per il successivo processamento.\n",
    "\n",
    "2. **Elaborazione in Tempo Reale**: Spark è in grado di gestire grandi volumi di dati e di elaborarli simultaneamente mentre arrivano.\n",
    "\n",
    "3. **Arricchimento dei Dati**: Durante l'elaborazione, Spark arricchisce i dati aggiungendo ulteriori informazioni. Nel progetto, questo include:\n",
    "    - **Aggiunta della stagione** in base alla data della misurazione, per differenziare i dati:\n",
    "      ```python\n",
    "      # Metodo per aggiungere la colonna stagione, mese, giorno e anno\n",
    "      def add_season_column(df):\n",
    "          # Estrazione mese, giorno e anno dal timestamp\n",
    "          df = df.withColumn(\"month\", month(col(\"Timestamp\")))\n",
    "          df = df.withColumn(\"day\", dayofmonth(col(\"Timestamp\")))\n",
    "          df = df.withColumn(\"year\", year(col(\"Timestamp\")))\n",
    "          # Inserimento della stagione in base al mese e al giorno\n",
    "          df = df.withColumn(\"season\",\n",
    "              when(\n",
    "                  ((col(\"month\") == 3) & (col(\"day\") >= 21)) |\n",
    "                  ((col(\"month\") > 3) & (col(\"month\") < 6)) |\n",
    "                  ((col(\"month\") == 6) & (col(\"day\") <= 20)),\n",
    "                  \"Primavera\"\n",
    "              ).when(\n",
    "                  ((col(\"month\") == 6) & (col(\"day\") >= 21)) |\n",
    "                  ((col(\"month\") > 6) & (col(\"month\") < 9)) |\n",
    "                  ((col(\"month\") == 9) & (col(\"day\") <= 20)),\n",
    "                  \"Estate\"\n",
    "              ).when(\n",
    "                  ((col(\"month\") == 9) & (col(\"day\") >= 21)) |\n",
    "                  ((col(\"month\") > 9) & (col(\"month\") < 12)) |\n",
    "                  ((col(\"month\") == 12) & (col(\"day\") <= 20)),\n",
    "                  \"Autunno\"\n",
    "              ).otherwise(\"Inverno\")\n",
    "          )\n",
    "          return df\n",
    "      ```\n",
    "    - **Classificazione della misurazione** in base al momento della giornata (giorno o notte), considerando l'orario dell'alba e del tramonto, che varia ogni giorno:\n",
    "      ```python\n",
    "      def determine_day_night(timestamp_str, lat, lon):\n",
    "          try:\n",
    "              # Conversione del timestamp in un oggetto datetime\n",
    "              timestamp = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S')\n",
    "              timestamp = utc.localize(timestamp)  # Assicurarsi che il timestamp sia in UTC\n",
    "              # Calcolare l'orario di alba e tramonto\n",
    "              sunrise = get_sunrise(lat, lon, timestamp.date())\n",
    "              sunset = get_sunset(lat, lon, timestamp.date())\n",
    "              # Estrazione dell'orario dal timestamp\n",
    "              current_time = timestamp.time()\n",
    "\n",
    "              # Confronta il tempo corrente con gli orari di alba e tramonto\n",
    "              if sunrise.time() <= current_time <= sunset.time():\n",
    "                  return 'giorno'\n",
    "              else:\n",
    "                  return 'notte'\n",
    "          except Exception as e:\n",
    "              # In caso di errore restituisce 'unknown'\n",
    "              print(f\"Errore nella funzione per determinare notte o giorno: {e}\")\n",
    "              return 'unknown'\n",
    "      ```\n",
    "\n",
    "4. **Output dei Dati**: Dopo l'elaborazione, i dati vengono preparati per essere inviati a Elasticsearch, dove saranno indicizzati e successivamente visualizzati attraverso Kibana. Spark si occupa anche di creare l'indice in Elasticsearch se non esiste già e di definire il suo mapping.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2b93ca",
   "metadata": {},
   "source": [
    "### Elasticsearch e Kibana\n",
    "\n",
    "Nel progetto **MATE**, **Elasticsearch** e **Kibana** hanno un ruolo fondamentale nella fase finale del processo di elaborazione e visualizzazione dei dati. Questi strumenti lavorano insieme per fornire una potente piattaforma di ricerca e visualizzazione dei dati elaborati da **Spark**.\n",
    "\n",
    "**Elasticsearch** è il nostro **Data Indexing** per gestire e indicizzare grandi volumi di dati in modo rapido ed efficiente. Dopo che i dati sono stati elaborati e arricchiti da Spark, vengono inviati a Elasticsearch, dove vengono memorizzati in indici. Questo processo richiede una notevole quantità di RAM.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"Img/Meme/meme_ES.jpg\" width=\"600\" style=\"height:auto;\" />\n",
    "</div> \n",
    "\n",
    "La configurazione dell’indice per Elasticsearch, creata da Spark, è la seguente:\n",
    "```json\n",
    "{\n",
    "  \"settings\": {\n",
    "    \"index\": {\n",
    "      \"number_of_shards\":5,\n",
    "      \"number_of_replicas\":1,\n",
    "      \"analysis\": {\n",
    "        \"analyzer\": {\n",
    "          \"custom_analyzer\": {\n",
    "            \"type\": \"custom\",\n",
    "            \"tokenizer\": \"standard\",\n",
    "            \"filter\": [\n",
    "              \"lowercase\",\n",
    "              \"asciifolding\"\n",
    "            ]\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"Timestamp\": {\n",
    "        \"type\": \"date\",\n",
    "        \"format\": \"yyyy-MM-dd HH:mm:ss\"\n",
    "      },\n",
    "      \"Temperature_Celsius\": {\n",
    "        \"type\": \"float\"\n",
    "      },\n",
    "      \"Relative_Humidity\": {\n",
    "        \"type\": \"float\"\n",
    "      },\n",
    "      \"temp_bin\": {\n",
    "        \"type\": \"integer\"\n",
    "      },\n",
    "      \"month\": {\n",
    "        \"type\": \"integer\"\n",
    "      },\n",
    "      \"day\": {\n",
    "        \"type\": \"integer\"\n",
    "      },\n",
    "      \"year\": {\n",
    "        \"type\": \"integer\"\n",
    "      },\n",
    "      \"season\": {\n",
    "        \"type\": \"keyword\"\n",
    "      },\n",
    "      \"day_night\": {\n",
    "        \"type\": \"keyword\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    " ```\n",
    "\n",
    "**Kibana** è invece il nostro  **Data Visualization**, cioè lo strumento di visualizzazione che si integra perfettamente con Elasticsearch. Permette di creare dashboard interattive e visualizzazioni personalizzate per esplorare i dati e ottenere approfondimenti significativi.\n",
    "Questa fase rappresenta il culmine del progetto, permettendo agli utenti di visualizzare e analizzare i dati secondo le proprie esigenze. Ad esempio, è possibile individuare correlazioni tra umidità e temperatura, analizzare la distribuzione percentuale delle temperature con categorizzazioni specifiche come giorno e notte, e applicare filtri dinamici per isolare set di dati specifici, consentendo un'analisi dettagliata e mirata.\n",
    "\n",
    "Allo stesso tempo, Kibana ci offre la possibilità di analizzare le tendenze stagionali in modo approfondito, utilizzando grafici a linee che mettono in relazione variabili come temperatura e umidità. Grazie alla possibilità di distinguere tra giorno e notte, l'analisi diventa più precisa, permettendo di identificare pattern specifici legati alle diverse fasi della giornata. Questo consente non solo di monitorare l’andamento climatico nel breve e lungo periodo, ma anche di evidenziare come le condizioni atmosferiche variano in funzione del ciclo giornaliero.\n",
    "\n",
    "Inoltre, Kibana permette di applicare filtri dinamici, offrendo agli utenti la libertà di concentrarsi su intervalli temporali specifici, stagioni particolari o di comparare i dati tra anni differenti. Questo livello di personalizzazione si rivela cruciale per individuare correlazioni non immediatamente visibili, come l'influenza dell'umidità sulle temperature medie stagionali o le variazioni significative durante le transizioni tra stagioni. \n",
    "\n",
    "Di seguito, puoi visualizzare il widget dello slider e le immagini delle varie visualizzazioni delle dashboard. Gli screenshot sono stati catturati il 10/09/2024 alle ore 19:20:\n",
    "\n",
    "**⚠️ Nota sui Widget Interattivi**: Il notebook usa `ipywidgets` per rendere le cose interattive e mostrare le immagini delle dashboard una alla volta. Su GitHub, però, i widget non funzionano e appariranno solo come codice. Se vuoi usarli davvero, scarica il notebook e aprilo in Jupyter Notebook. In caso contrario, consiglio di dare un'occhiata alla cartella Dashbord.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "652b9606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e985b7cdbef445c080ec2f81837c1162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=0, continuous_update=False, description='Immagine:', max=22)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d73e66d4e3db49519d35227208898c65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Image, Markdown\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Lista delle immagini e delle descrizioni\n",
    "images = [\n",
    "    \"Img/Dashbord/Ultimi 2 ore.png\",\n",
    "    \"Img/Dashbord/Ultimi 12 ore.png\",\n",
    "    \"Img/Dashbord/Ultimi 16 ore.png\",\n",
    "    \"Img/Dashbord/Oggi.png\",\n",
    "    \"Img/Dashbord/09-09-2024(pioggia).png\",\n",
    "    \"Img/Dashbord/Ultimi 7 giorni.png\",\n",
    "    \"Img/Dashbord/Ultimi 30 giorni.png\",\n",
    "    \"Img/Dashbord/Ultimi 90 giorni.png\",\n",
    "    \"Img/Dashbord/Ultimo Anno Primavera Autunno.png\",\n",
    "    \"Img/Dashbord/Ultimo Anno Inverno Estate.png\",\n",
    "    \"Img/Dashbord/Ultimo Anno Inverno.png\",\n",
    "    \"Img/Dashbord/Ultimo Anno Autunno.png\",\n",
    "    \"Img/Dashbord/Ultimo Anno Estate.png\",\n",
    "    \"Img/Dashbord/Ultimo Anno Primavera.png\",\n",
    "    \"Img/Dashbord/Ultimo Anno.png\",\n",
    "    \"Img/Dashbord/Due Anni.png\",\n",
    "    \"Img/Dashbord/Tre anni Primavera Autunno.png\",\n",
    "    \"Img/Dashbord/Tre anni estate inverno.png\",\n",
    "    \"Img/Dashbord/Tre anni Primavera.png\",\n",
    "    \"Img/Dashbord/Tre anni inverno.png\",\n",
    "    \"Img/Dashbord/Tre anni autunno.png\",\n",
    "    \"Img/Dashbord/Tre anni estate.png\",\n",
    "    \"Img/Dashbord/Tre anni.png\"\n",
    "]\n",
    "\n",
    "descriptions = [\n",
    "    \"Visualizzazione dei dati raccolti nelle ultime 2 ore. Questa immagine mostra i dettagli recenti per un'analisi tempestiva.\",\n",
    "    \"Dashboard che rappresenta i dati aggregati delle ultime 12 ore. Utile per monitorare le tendenze a breve termine.\",\n",
    "    \"Panoramica dei dati raccolti nelle ultime 16 ore, evidenziando le fluttuazioni e le variazioni nel tempo.\",\n",
    "    \"Dati del giorno corrente (10/09/2024), fornendo una visione chiara e dettagliata delle condizioni attuali.\",\n",
    "    \"Visualizzazione dei dati raccolti il 09/09/2024, con particolare attenzione alle condizioni meteorologiche di pioggia.\",\n",
    "    \"Visualizzazione dei dati degli ultimi 7 giorni, utile per identificare tendenze settimanali e anomalie.\",\n",
    "    \"Panoramica dei dati raccolti negli ultimi 30 giorni, consentendo un'analisi mensile e l'identificazione di schemi stagionali.\",\n",
    "    \"Dashboard che mostra i dati per gli ultimi 90 giorni, fornendo una visione estesa per analisi trimestrali e rilevamento di trend a lungo termine.\",\n",
    "    \"Distribuzione dei dati per le stagioni di Primavera e Autunno nell'ultimo anno, utile per osservare le variazioni stagionali annuali.\",\n",
    "    \"Panoramica dei dati stagionali di Inverno ed Estate nell'ultimo anno, evidenziando le differenze tra i due periodi dell'anno.\",\n",
    "    \"Visualizzazione dei dati raccolti durante l'Inverno nell'ultimo anno, utile per analizzare le tendenze invernali.\",\n",
    "    \"Dati autunnali raccolti nell'ultimo anno, fornendo una panoramica delle condizioni tipiche di questa stagione.\",\n",
    "    \"Distribuzione dei dati estivi raccolti nell'ultimo anno, evidenziando le caratteristiche delle condizioni estive.\",\n",
    "    \"Panoramica dei dati primaverili dell'ultimo anno, utile per analizzare le tendenze e i cambiamenti durante la primavera.\",\n",
    "    \"Visualizzazione complessiva dei dati per l'intero anno, consentendo un'analisi annuale completa e la comparazione tra le diverse stagioni.\",\n",
    "    \"Dati raccolti nell'ultimo biennio, fornendo una visione a lungo termine delle condizioni meteorologiche.\",\n",
    "    \"Visualizzazione dei dati primaverili e autunnali degli ultimi tre anni, utile per osservare le tendenze a lungo termine in queste due stagioni.\",\n",
    "    \"Distribuzione dei dati estivi e invernali degli ultimi tre anni, fornendo una panoramica comparativa tra i due periodi.\",\n",
    "    \"Panoramica dei dati primaverili degli ultimi tre anni, evidenziando le variazioni stagionali nel corso degli anni.\",\n",
    "    \"Distribuzione dei dati invernali degli ultimi tre anni, utile per analizzare le tendenze e i pattern durante l'inverno.\",\n",
    "    \"Visualizzazione dei dati autunnali degli ultimi tre anni, mostrando le tendenze e i cambiamenti stagionali in questo periodo.\",\n",
    "    \"Dati estivi raccolti negli ultimi tre anni, fornendo una visione dettagliata delle condizioni estive in un arco temporale esteso.\",\n",
    "    \"Panoramica dei dati raccolti negli ultimi tre anni, consentendo un'analisi a lungo termine e la comparazione tra i diversi anni.\"\n",
    "]\n",
    "\n",
    "\n",
    "# Funzione per mostrare l'immagine e la descrizione\n",
    "def show_image(index):\n",
    "    display(Markdown(f\"**Descrizione:** {descriptions[index]}\"))\n",
    "    display(Image(filename=images[index]))\n",
    "\n",
    "# Widget per il controllo della galleria\n",
    "image_slider = widgets.IntSlider(\n",
    "    value=0,\n",
    "    min=0,\n",
    "    max=len(images)-1,\n",
    "    step=1,\n",
    "    description='Immagine:',\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "# Output\n",
    "output = widgets.Output()\n",
    "\n",
    "# Collega il widget all'output\n",
    "def update_image(change):\n",
    "    with output:\n",
    "        output.clear_output(wait=True)\n",
    "        show_image(image_slider.value)\n",
    "\n",
    "image_slider.observe(update_image, names='value')\n",
    "update_image(None)  # Mostra l'immagine iniziale\n",
    "\n",
    "# Visualizza il widget e l'output\n",
    "display(image_slider, output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0183670d",
   "metadata": {},
   "source": [
    "### Portainer\n",
    "Una menzione speciale va a **Portainer**, che ha facilitato notevolmente la gestione dei container nel contesto dello sviluppo del progetto. Grazie a Portainer, è stato possibile monitorare lo stato dei vari container, gestirne l'avvio, l'arresto e persino ricrearli completamente da zero con facilità\n",
    "\n",
    "Inoltre, Portainer si è rivelato fondamentale durante la fase di sviluppo grazie alla possibilità di visualizzare i log in tempo reale per ogni container. Questa funzionalità ha permesso di individuare rapidamente gli errori e diagnosticare problematiche specifiche all'interno dei container, come nel caso frequente in cui Elasticsearch terminava con errore 137, legato all'esaurimento delle risorse disponibili. La capacità di monitorare questi eventi ha velocizzato il processo di debugging, migliorando l'efficienza e l'affidabilità del sistema nel complesso.\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"Img/Meme/meme_portainer.jpg\" width=\"600\" style=\"height:auto;\" />\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cdb3dd",
   "metadata": {},
   "source": [
    "## 3. Conclusioni\n",
    "In conclusione, il progetto **MATE** ha dimostrato come diverse tecnologie possano interaggire efficacemente tra di loro. Senza **Logstash**, non avremmo potuto acquisire i nostri dati (CSV e REST). Senza **Kafka**, non sarebbe stato possibile trasmettere tali dati a **Spark**, a causa dell'elevato volume di dati e del tempo necessario per elaborarli; questo è stato reso possibile grazie all'asincronia offerta da Kafka. **Spark** ha poi arricchito i dati, dimostrandosi uno strumento estremamente potente, gestendo oltre 1.500.000 misurazioni. Anche **Elasticsearch** ha svolto un ruolo fondamentale, indicizzando tutti questi dati, mentre **Kibana** ha reso possibile trasformare questo flusso di dati confusi in dashboard intuitive e chiare.\n",
    "\n",
    "In breve, tecnologie diverse hanno lavorato insieme per creare un unico *meccanismo* efficace.\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"Img/Diagrammi/Diagramma_finale.png\" width=\"300\" style=\"height:auto;\" />\n",
    "</div> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
